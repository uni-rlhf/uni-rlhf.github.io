<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners."
    />
    <meta name="keywords" content="Reinforcement Learning, Diffusion Model, Trajectory Optimization" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback
    </title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="./public/index.css" rel="stylesheet" />
    <link href="./public/media.css" rel="stylesheet" />
    <link href="./public/sidebars.css" rel="stylesheet" />
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="./public/js/base.js"></script>
  </head>

  <body>
    <div class="sidebarsWrapper">
      <div class="sidebars">
        <a class="barWrapper" clear href="#abstract-a" id="bar2"
          ><span>Abstract</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#tr2-a" id="bar3"
          ><span>Trajectory Translation</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#results-a" id="bar4"
          ><span>Results</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#attn-a" id="bar5"
          ><span>Attention Analysis</span>
          <div class="bar"></div
        ></a>
      </div>
    </div>
    <main class="content">
      <section class="heading">
        <h1 class="title">
          Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback
        </h1>
        <section class="authors">
          <ul>
          </ul>
        </section>
        <section class="affiliations">
          <ul>
          </ul>
        </section>
        <section class="links">
          <ul>
            <a href="https://arxiv.org/abs/2302.01877" rel="noreferrer" target="_blank">
              <li>
                <span class="icon"> <img src="./public/paper.svg" /> </span
                ><span>Paper</span>
              </li>
            </a>
<!--            <a-->
<!--              href="https://youtu.be/ZjRYlRRV9IA"-->
<!--              rel="noreferrer"-->
<!--              target="_blank"-->
<!--            >-->
<!--              <li>-->
<!--                <span class="icon"> <img src="./public/video.svg" /> </span-->
<!--                ><span>Video</span>-->
<!--              </li>-->
<!--            </a>-->
            <a
              href="https://github.com/TJU-DRL-LAB/Enhanced-RLHF/tree/demo"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon">
                  <img src="./public/github.svg" />
                </span>
                <span>Code</span>
              </li>
            </a>
            <!-- <a><li>Video</li></a> -->
          </ul>
        </section>
        <a class="anchor" id="abstract-a"></a>
        <h2>Abstract</h2>
        <p class="abstract" style="font-family: 'Times New Roman', Arial;">
          <!-- Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art <a href="https://diffusion-planning.github.io/" rel="noreferrer" target="_blank">Diffuser</a> by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data. -->

          Reinforcement Learning with Human Feedback~(RLHF) has received significant attention to perform tasks without costly manual reward design by aligning human preferences. It is crucial to consider different sources of human feedback types and learning methods in different environments. However, it is difficult to quantify the progress in RLHF with diverse feedback due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce \textbf{\alg}, a comprehensive system implementation tailored for RLHF, aiming to provide complete workflow from \textit{real human feedback}, fostering progress in the development practical problems. \alg contains three packages: 1) universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. \alg has developed an user-friendly annotation interface tailored to various feedback types, with the ability to be compatible with a wide range of mainstream RL environments. Then, we developed a systematic pipeline of crowdsourced annotations, resulting in a large-scale annotated datasets of more than 15 million steps across 32 popular tasks. Through extensive experiments, the results in the collected datasets achieve competitive performance compared to well-designed manual reward. We evaluate some design choices and offer insights into their strengths and potential areas of improvement. We wish to build valuable open-source platform, datasets and baselines to facilitate develop more robust and reliable RLHF solutions based on realistic human feedback. The benchmark website is available at \url{uni-rlhf.github.io}.
        </p>
      </section>
      
    </main>
  </body>
</html>
