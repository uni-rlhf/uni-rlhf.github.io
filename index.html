<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners."
    />
    <meta name="keywords" content="Reinforcement Learning, Diffusion Model, Trajectory Optimization" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL
    </title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="./public/index.css" rel="stylesheet" />
    <link href="./public/media.css" rel="stylesheet" />
    <link href="./public/sidebars.css" rel="stylesheet" />
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="./public/js/base.js"></script>
  </head>

  <body>
    <div class="sidebarsWrapper">
      <div class="sidebars">
        <a class="barWrapper" clear href="#abstract-a" id="bar2"
          ><span>Abstract</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#tr2-a" id="bar3"
          ><span>Trajectory Translation</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#results-a" id="bar4"
          ><span>Results</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#attn-a" id="bar5"
          ><span>Attention Analysis</span>
          <div class="bar"></div
        ></a>
      </div>
    </div>
    <main class="content">
      <section class="heading">
        <h1 class="title">
          MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL
        </h1>
        <section class="authors">
          <ul>
            <li>
              <span
                ><a
                  href="https://fei-ni.github.io"
                  rel="noreferrer"
                  target="_blank"
              >Fei Ni</a
                ><sup>1</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="http://www.icdai.org/jianye.html/"
                  rel="noreferrer"
                  target="_blank"
                  >Jianye Hao</a
                ><sup>1 3</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://yaomarkmu.github.io/"
                  rel="noreferrer"
                  target="_blank"
                  >Yao Mu</a
                ><sup>2</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://scholar.google.com/citations?hl=zh-CN&user=83JhosMAAAAJ"
                  rel="noreferrer"
                  target="_blank"
                  >Yifu Yuan</a
                ><sup>1</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://yanzzzzz.github.io"
                  rel="noreferrer"
                  target="_blank"
                  >Yan Zheng</a
                ><sup>1</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://scholar.google.com.hk/citations?user=KWZG_YsAAAAJ&hl=zh-CN&oi=sra"
                  rel="noreferrer"
                  target="_blank"
                  >Bin Wang</a
                ><sup>3</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://liang-zx.github.io/"
                  rel="noreferrer"
                  target="_blank"
                  >Zhixuan Liang</a
                ><sup>2</sup></span
              >
            </li>
          </ul>
        </section>
        <section class="affiliations">
          <ul>
            <span class="author-block"><sup>1</sup>Tianjin University,</span>
            <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>3</sup>Huawei Noah's Ark Lab</span>
          </ul>
        </section>
        <section class="links">
          <ul>
            <a href="https://arxiv.org/abs/2302.01877" rel="noreferrer" target="_blank">
              <li>
                <span class="icon"> <img src="./public/paper.svg" /> </span
                ><span>Paper</span>
              </li>
            </a>
<!--            <a-->
<!--              href="https://youtu.be/ZjRYlRRV9IA"-->
<!--              rel="noreferrer"-->
<!--              target="_blank"-->
<!--            >-->
<!--              <li>-->
<!--                <span class="icon"> <img src="./public/video.svg" /> </span-->
<!--                ><span>Video</span>-->
<!--              </li>-->
<!--            </a>-->
            <a
              href="https://github.com/Liang-ZX/adaptdiffuser"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon">
                  <img src="./public/github.svg" />
                </span>
                <span>Code</span>
              </li>
            </a>
            <!-- <a><li>Video</li></a> -->
          </ul>
        </section>
        <a class="anchor" id="abstract-a"></a>
        <h2>Abstract</h2>
        <p class="abstract" style="font-family: 'Times New Roman', Arial;">
          <!-- Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art <a href="https://diffusion-planning.github.io/" rel="noreferrer" target="_blank">Diffuser</a> by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data. -->

          Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning. However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.
        </p>
      </section>
      <br />
      <section class="head-media">

          <div style="display: flex; margin: auto; width: 95%">
          <img
          style="width: 90%"
          src="./public/images/motivation.png"
          />
          
        </div>
<!--         <div style="display: flex; width: 90%; margin: auto">
          &nbsp;&nbsp;&nbsp;&nbsp;
          <img
          style="width: 60%"
          src="./public/images/motivation.png"
          />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
          <p class="caption" style="text-align: justify; font-family: 'Times New Roman'"><br />
            <strong>Motivation overview of MetaDiffuser.</strong> It enables diffusion models to generate rich synthetic expert data using guidance from reward gradients of either seen or unseen goal-conditioned tasks. Then, it iteratively selects high-quality data via a discriminator to finetune the diffusion model for self-evolving, leading to improved performance on seen tasks and better generalizability to unseen tasks.
            Planning with diffusion model (Janner et al., 2022b) provides a promising paradigm for offline RL, which utilizes
diffusion model as a trajectory generator by joint diffusing the states and actions from the noise to formulate the
sequence decision-making problem as standard generative
modeling. 
          </p>

        </div>
      </section>
      <br />
<!--      <section class="head-media">-->
<!--        <video autoplay="" muted="" loop="" height="100%">-->
<!--          <source-->
<!--            src="./public/videos/castle-stack-medium.mp4"-->
<!--            type="video/mp4"-->
<!--          />-->
<!--        </video>-->
<!--        <video autoplay="" muted="" loop="" height="100%">-->
<!--          <source src="./public/videos/creeperonsnow.mp4" type="video/mp4" />-->
<!--        </video>-->
<!--        <video autoplay="" muted="" loop="" height="100%">-->
<!--          <source src="./public/videos/snowgolem.mp4" type="video/mp4" />-->
<!--        </video>-->
<!--        <video autoplay="" muted="" loop="" height="100%">-->
<!--          <source src="./public/videos/torch.mp4" type="video/mp4" />-->
<!--        </video>-->
<!--        <br />-->
<!--        <p class="caption">-->
<!--          Videos of succesful long-horizon block stacking tasks performed in the-->
<!--          real-world using a trajectory translation policy trained in-->
<!--          simulation. Tasks are unseen and require manipulation of blocks in-->
<!--          locations beyond the original training distribution.-->
<!--        </p>-->
<!--      </section>-->
      <a class="anchor" id="tr2-a"></a>
      <section class="details">
        <h2>Framework of MetaDiffuser</h2>
        <!-- <p>
          The core idea of our work is to simplify task-solving by enabling the
          low-level agent to explicitly focus only on low-level skills such as
          object manipulation and leave the high-level planning to be solved by
          a high-level agent. The objective of the low-level agent now is to
          simply follow the high-level agent as closely as possible.
        </p> -->
        <p style="font-family: 'Times New Roman'""> The overview of MetaDiffuser. During meta-training phase, a task-oriented context encoder is trained jointly with conditioned
dynamics model and reward model in a self-supervised manner to infer the current task from the recent historical transitions. Then, the
multi-task trajectories can be labeled with the trained context encoder and the inferred context are injected in the conditioned diffusion
model to estimating the multi-modal distribution mixed by different training tasks. During meta-testing phase, context encoder can
capture the task information from provided warm-start data from the test task. Then the conditioned diffusion model can manipulate the
noise model to denoise out desired trajectories for the test task with the inferred context. Additionally, the pretrained dynamics model and
reward model can serve as classifiers for evaluation, with gradient to guide the conditional generation in a classifier-guide fashion.

        </p>

        <div style="display: flex; margin: auto; width: 95%">
          <img
          style="width: 90%"
          src="./public/images/framework.png"
          />
          
          <!-- br />
          <img
          style="width: 50%;%"
          src="./public/video/video1.gif"
          />  -->
        </div>


      <a class="anchor" id="tr2-a"></a>
      <section class="details">
        <h2>Dual guide in the Trajectory Generation</h2>
        <!-- <p>
          The core idea of our work is to simplify task-solving by enabling the
          low-level agent to explicitly focus only on low-level skills such as
          object manipulation and leave the high-level planning to be solved by
          a high-level agent. The objective of the low-level agent now is to
          simply follow the high-level agent as closely as possible.
        </p> -->
        <p style="font-family: 'Times New Roman'""> Previous work (Janner et al., 2022a) trains an extra reward predictor to evaluate the accumulative return of generated trajectories and utilizes the gradient of return as a guidance in the sampling process of diffusion model, to encourage the generated trajectories to achieve high return. However, during meta-testing for unseen tasks, the conditional generated trajectories may not always obey dynamics constraints due to the aggressive guidance aim for high return, making it difficult for the planner to follow the expected trajectories during the interaction with the environment. Therefore, we propose a dual-guide to enhance the dynamics consistency of generated trajectories while encouraging the high return simultaneously.

        </p>

        <div style="display: flex; margin: auto; width: 95%">
          <p style="font-family: 'Times New Roman'; width: 25%;%">       <strong>Generated Trajectories.</strong>  </p>
          <p style="font-family: 'Times New Roman'; width: 25%;%">       <strong>Real Trajecories.</strong> </p>
          <p style="font-family: 'Times New Roman'; width: 25%;%">       <strong>Generated Trajectories.</strong>  </p>
          <p style="font-family: 'Times New Roman'; width: 25%;%">       <strong>Real Trajecories.</strong> </p>
        </div>


        <div style="display: flex; margin: auto; width: 95%">
          <!-- <img
          style="width: 90%"
          src="./public/images/framework.png"
          /> -->
          
          <!-- br /> -->
          <img
          style="width: 40%;%"
          src="./public/images/walker_without.gif"
          /> 
          <img
          style="width: 40%;%"
          src="./public/images/walker_with.gif"
          /> 

        </div>
          <br />
        <div style="display: flex; margin: auto; width: 95%">
          <p style="font-family: 'Times New Roman'; width: 50%;%">       Walker-Params without dual-guide. </p>
          <p style="font-family: 'Times New Roman'; width: 50%;%">       Walker-Params with dual-guide. </p>

        </div>

        <div style="display: flex; margin: auto; width: 95%">
          <!-- <img
          style="width: 90%"
          src="./public/images/framework.png"
          /> -->
          
          <!-- br /> -->
          <img
          style="width: 40%;%"
          src="./public/images/hopper_without.gif"
          /> 
          <img
          style="width: 40%;%"
          src="./public/images/hopper_with.gif"
          /> 

        </div>
          <br />
        <div style="display: flex; margin: auto; width: 95%">
          <p style="font-family: 'Times New Roman'; width: 50%;%">       Hopper-Params without dual-guide. </p>
          <p style="font-family: 'Times New Roman'; width: 50%;%">       Hopper-Params with dual-guide. </p>

        </div>

        <div style="display: flex; margin: auto; width: 95%">
          <!-- <img
          style="width: 90%"
          src="./public/images/framework.png"
          /> -->
          
          <!-- br /> -->
          <img
          style="width: 40%;%"
          src="./public/images/cheetah_without.gif"
          /> 
          <img
          style="width: 40%;%"
          src="./public/images/cheetah_with.gif"
          /> 

        </div>
          <br />
        <div style="display: flex; margin: auto; width: 95%">
          <p style="font-family: 'Times New Roman'; width: 50%;%">       Cheetah-Vel without dual-guide. </p>
          <p style="font-family: 'Times New Roman'; width: 50%;%">       Cheetah-Vel with dual-guide. </p>

        </div>

        <!-- <h2>Method</h2>
        <p>
          Below shows an illustration of the abstract-to-executable trajectory
          translation architecture. All
          <span style="color: blue">high-level states</span> from the abstract
          trajectory are fed through one encoder and the past k
          <span style="color: orange">low-level states</span> are fed through a
          separate encoder to create tokens. The tokens form a sequence that is
          given to the transformer model, and the final output embedding z<sub
            >n+k-1</sub
          >
          is passed through an MLP to produce
          <span style="color: red">actions</span>.
        </p>
        <img
          style="width: 100%"
          src="./public/translation_transformer_architecture.png"
        />
        <p>
          This model serves as the backbone and we train a policy using PPO and
          a general trajectory following reward (detailed in equation 1 of the
          paper) that encourages the agent to mimic the abstract trajectory as
          closely as possible.
        </p> -->
<!--        <a class="anchor" id="results-a"></a>-->
<!--        <h2>Results</h2>-->
<!--        <p>-->
<!--          We show example translations of abstract trajectories executed-->
<!--          trajectories below as well as detail the environments used and domain-->
<!--          gaps bridged. The left column of videos shows the abstract trajectory-->
<!--          and the right column shows the executed trajectory. The high-level-->
<!--          agents are written using simple heuristics and are represented as a-->
<!--          point mass floating in 2D/3D space. As the abstract trajectory lacks-->
<!--          low-level details, the low-level agent must learn and discover these-->
<!--          details such as object manipulation and apply them while mimicing the-->
<!--          abstract trajectory. Furthermore, re-planning is a feasible feature as-->
<!--          abstract trajectories can be re-generated to handle mistakes or-->
<!--          external interventions.-->
<!--          &lt;!&ndash; Note that while objects like blocks and drawers are rendered in the abstract trajectory display, the abstract trajectory itself only contains 3D position information. &ndash;&gt;-->
<!--        </p>-->

<!--        <div class="abstractexecutable">-->
<!--          <p>-->
<!--            Show abstract-to-executable translations on-->
<!--            <select id="task-select">-->
<!--              <option>Test Tasks</option>-->
<!--              <option>Train Tasks</option>-->
<!--            </select>-->
<!--          </p>-->
<!--          <div class="col-title">-->
<!--            <p>Abstract Trajectory</p>-->
<!--            <p>Executed Trajectory</p>-->
<!--          </div>-->
<!--          <div>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/box_high_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/box_low_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <p>-->
<!--              <strong>Box Pusher</strong> <br />-->
<!--              The training task is to control an agent (black box) to move a-->
<!--              green box to a target (blue sphere). The high-level agent can-->
<!--              magically grasp and thus drag the green box. However, the-->
<!--              low-level agent is restricted to only pushing and must process the-->
<!--              abstract trajectory to determine which direction to push the green-->
<!--              box in. At test time there are obstacles observable only by the-->
<!--              high-level agent.-->
<!--            </p>-->
<!--          </div>-->
<!--          <div>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/couch_moving_high_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/couch_moving_low_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <p>-->
<!--              <strong>Couch Moving</strong> <br />-->
<!--              The training task is to move the couch shaped agent through a map-->
<!--              of chambers and corners. The agent's couch morphology means that-->
<!--              the agent must rotate in chambers ahead of time in order to go-->
<!--              through corners. The high-level agent simply tells the low-level-->
<!--              agent the path through the map, indicating where corners are, but-->
<!--              it is up to the low-level agent to process this information to-->
<!--              determine when to rotate in chambers. At test time, maps are-->
<!--              longer and vary more.-->
<!--            </p>-->
<!--          </div>-->
<!--          <div>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/stack_high_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/stack_low_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <p>-->
<!--              <strong>Block Stacking</strong> <br />-->
<!--              The training task is to stack a block with a robot arm. The-->
<!--              high-level agent can magically grasp and release blocks anywhere-->
<!--              and move easily through space. The low-level agent must process-->
<!--              the abstract trajectory to determine where to pick up the block-->
<!--              and where to stack it. At test time an agent has to stack multiple-->
<!--              blocks in a row in locations beyond the training distribution.-->
<!--            </p>-->
<!--          </div>-->
<!--          <div>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/drawer_high_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <video autoplay="" muted="" loop="" height="100%">-->
<!--              <source-->
<!--                src="./public/videos/drawer_low_level.mp4"-->
<!--                type="video/mp4"-->
<!--              />-->
<!--            </video>-->
<!--            <p>-->
<!--              <strong>Open Drawer</strong> <br />-->
<!--              The training task is open various drawers on cabinets with a-->
<!--              mobile robot arm. The high-level agent can magically grasp and-->
<!--              pull open drawers easily. The low-level agent must process the-->
<!--              abstract trajectory to determine how to follow the abstract-->
<!--              trajectory and how to pull open the drawer. At test time the agent-->
<!--              must open unseen drawers with unseen handles as well as open more-->
<!--              than one drawer on a cabinet.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->
<!--        <a class="anchor" id="attn-a"></a>-->
<!--        <h2>Attention Analysis</h2>-->
<!--        <p>-->
<!--          To get an insight into how the transformer architecture enables the-->
<!--          policy to solve environments more succesfully, we analyze the learned-->
<!--          attention on the Couch Moving environment.-->
<!--        </p>-->
<!--        <div class="attn-video" style="text-align: center">-->
<!--          <video autoplay="" muted="" loop="" height="100%" style="width: 75%">-->
<!--            <source src="./public/videos/attn.mp4" type="video/mp4" />-->
<!--          </video>-->
<!--        </div>-->
<!--        <p>-->
<!--          In Couch Moving, the abstract trajectory is composed of high-level-->
<!--          states which are simply the 2D position of the high-level agent moving-->
<!--          through the maze. We can treat this as a map and easily visualize it-->
<!--          over the map. The above video shows the attention over the abstract-->
<!--          trajectory / map as the agent solves the task, with dark blue-->
<!--          representing high attention and light blue representing minimal-->
<!--          attention. We observe that when the agent is in a chamber, it learns-->
<!--          to pay attention to the next or next next chamber, both of which are-->
<!--          indicative of which orientation the next corner is in. With this-->
<!--          attention, the agent is capable of making the correct decision on-->
<!--          whether to rotate or not in order to move through the next corner.-->
<!--          Results show that transformer architectures achieves much higher success-->
<!--          rates compared to LSTM architectures or sub-goal conditioned policies.-->
<!--        </p>-->
<!--      </section>-->
      <!-- <section class="citation">
        <h2>Bibtex</h2>
        <pre><code>@article{liang2023adaptdiffuser,
          title={AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners},
          author={Liang, Zhixuan and Mu, Yao and Ding, Mingyu and Ni, Fei and Tomizuka, Masayoshi and Luo, Ping},
          journal={arXiv preprint arXiv:2302.01877},
          year={2023}
        }</code></pre>
      </section> -->
<!--      <section class="acknowledgements">-->
<!--        <h2>Acknowledgements</h2>-->
<!--        <p>-->
<!--          Special thanks to Jiayuan Gu for feedback on figures, and additional-->
<!--          members of the SU Lab for writing feedback.-->
<!--        </p>-->
<!--      </section>-->
    </main>
  </body>
</html>
