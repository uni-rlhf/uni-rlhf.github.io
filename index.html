<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Uni-RLHF | Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"><img src="./assets/images/logo.png" alt="Logo" style="margin-right: 1px; width: 70px;margin-top: 0px;">Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback</h1>
<!--                        <h1 class="title is-1 publication-title">Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback</h1>-->
                        <h2 class="title is-3 conference-authors"><a target="_blank" href="">ICLR 2024</a></h2>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://yifu-yuan.github.io">
                                    Yifu&#160;Yuan</a>
                                <sup>1</sup>,
                                <a target="_blank"
                                    href="http://www.icdai.org/jianye.html">Jianye&#160;Hao</a>
                                <sup>1,2,&#8225</sup>
                                <a target="_blank"
                                    href="https://mayi1996.top/">Yi&#160;Ma</a><sup>1</sup>,
                                <a target="_blank"
                                    href="https://zibindong.github.io/">Zibin&#160;Dong</a><sup>1</sup>,
                                <a target="_blank"
                                    href="">Hebin&#160;Liang</a><sup>1</sup>,
                                <br>
                                <a target="_blank"
                                    href="">Jinyi&#160;Liu</a><sup>1</sup>,
                                <a target="_blank"
                                    href="">Zhixin&#160;Feng</a><sup>1</sup>,
                                <a target="_blank"
                                    href="">Kai&#160;Zhao</a><sup>1</sup>,
                                <a target="_blank"
                                    href="https://yanzzzzz.github.io/">Yan&#160;Zheng</a><sup>1</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Tianjin University; </span>
                            <span class="author-block"><sup>2</sup>Huawei Noah's Ark Lab; </span>
                            <span class="author-block"><sup>&#8225;</sup>Corresponding Author</span>
                        </div>
<!--                        <div class="is-size-5 publication-authors">-->

<!--                        <span class="author-block">-->
<!--                                    <span class="icon">-->
<!--                                        <i class="fa fa-envelope"></i>-->
<!--                                    </span>-->
<!--                            Corresponding authors: jasonyma@seas.upenn.edu, dr.jimfan.ai@gmail.com-->
<!--                        </span>-->
<!--                        </div>-->

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- TODO REPLACE ALL LINKS -->
                                <span class="link-block">
                                    <a target="_blank" href="https://arxiv.org/abs/2402.02423"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Arxiv</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a target="_blank" href="https://drive.google.com/drive/folders/1JMWyl0iAm2JJ5pOBW5M9kTOj6pJn8H3N?usp=drive_link"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                    <a target="_blank" href="https://github.com/pickxiguapi/Uni-RLHF-Platform"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Platform</span>
                                    </a>
                                    <a target="_blank" href="https://github.com/pickxiguapi/Clean-Offline-RLHF"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Clean Offline RLHF</span>
                                    </a>
                                </span>

<!--                                <span class="link-block">-->
<!--                                    <a target="_blank" href="assets/eureka_paper.pdf"-->
<!--                                        class="external-link button is-normal is-rounded is-dark">-->
<!--                                        <span class="icon">-->
<!--                                            <i class="fas fa-file-pdf"></i>-->
<!--                                        </span>-->
<!--                                        <span>PDF</span>-->
<!--                                    </a>-->
<!--                                    <a target="_blank" href="https://huggingface.co/datasets/VIMA/VIMA-Data"-->
<!--                                        class="external-link button is-normal is-rounded is-dark">-->
<!--                                        <span class="icon">-->
<!--                                            <i class="fas fa-database"></i>-->
<!--                                        </span>-->
<!--                                        <span>Dataset</span>-->
<!--                                    </a>-->
<!--                                </span>-->
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
<!--    <div class="columns is-centered has-text-centered">-->
<!--        <div class="column">-->
<!--            <p style="font-size: 125%">-->
<!--                <i>We provide a test account for the demo. <strong>Username: test Password: test</strong></i>-->
<!--            </p>-->
<!--        </div>-->
<!--    </div>-->


<!--    <section class="section" style="padding: 0">-->
<!--        <div class="container is-max-desktop">-->
<!--            &lt;!&ndash; Abstract. &ndash;&gt;-->
<!--            <div class="columns is-centered has-text-centered">-->
<!--                <video poster="" id="" autoplay controls muted loop width="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                    <source src="videos/eureka_zoomout.mp4" type="video/mp4">-->
<!--                </video>-->
<!--            </div>-->
<!--        </div>-->
<!--    </section>-->

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce <strong>Uni-RLHF</strong>, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from <i>real human feedback</i>, fostering progress in the development of practical problems. Uni-RLHF contains three packages: <strong>1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations.</strong> Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of mainstream RL environments. We then establish a systematic pipeline of crowdsourced annotations, resulting in large-scale annotated datasets comprising more than 15 million steps across 32 popular tasks. Through extensive experiments, the results in the collected datasets demonstrate competitive performance compared to those from well-designed manual rewards. We evaluate various design choices and offer insights into their strengths and potential areas of improvement. We wish to build valuable open-source platforms, datasets, and baselines to facilitate the development of more robust and reliable RLHF solutions based on realistic human feedback.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

<!--    <section class="section">-->
<!--        <div class="container is-max-widescreen">-->
<!--            <div class="rows">-->
<!--                <div class="rows is-centered ">-->
<!--                    <div class="row is-full-width">-->
<!--                        <h2 class="title is-3"><span class="dvima">Eureka Rewards and Policies</span></h2>-->
<!--                        <p style="font-size: 125%">-->
<!--                            In this demo, we visualize the unmodified best Eureka reward-->
<!--                            for each-->
<!--                            environment and the policy trained using this reward. Our environment suite spans 10 robots and 29 distinct tasks across two -->
<!--                            open-sourced benchmarks, Isaac Gym (Isaac) and Bidexterous Manipulation (Dexterity).-->
<!--                        </p>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--            <br>-->
<!--            <div class="columns" style="align-items: end;">-->
<!--                <div class="column is-half">-->
<!--                    <p style="font-size: 1.5em"><b>Isaac</b></p>-->
<!--                    <div class="col-md-4 col-sm-4 col-xs-4">-->
<!--                        <img src="videos/task_final/allegro_hand.png" width="18%" style="border-radius: 5px;" alt='<b>AllegroHand</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/allegro_hand.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/ant.png" width="18%" style="border-radius: 5px;" alt='<b>Ant</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/ant.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/anymal.png" width="18%" style="border-radius: 5px;" alt='<b>Anymal</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/anymal.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/ball_balance.png" width="18%" style="border-radius: 5px;" alt='<b>BallBalance</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/ball_balance.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/cartpole.png" width="18%" style="border-radius: 5px;" alt='<b>Cartpole</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/cartpole.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/franka_cabinet.png" width="18%" style="border-radius: 5px;" alt='<b>FrankaCabinet</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/franka_cabinet.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/humanoid.png" width="18%" style="border-radius: 5px;" alt='<b>Humanoid</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/humanoid.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/quadcopter.png" width="18%" style="border-radius: 5px;" alt='<b>Quadcopter</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/quadcopter.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/shadow_hand.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHand</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand.txt' onclick="populateDemo(this, 1);">-->
<!--                    </div>-->

<!--                    <br>-->
<!--                    <p style="font-size: 1.5em"><b>Dexterity</b></p>-->
<!--                    <div class="col-md-4 col-sm-4 col-xs-4">-->
<!--                        <img src="videos/task_final/bidex_block_stacking.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandBlockStack</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_block_stack.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_bottle_cap.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandBottleCap</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_bottle_cap.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_catch_abreast.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandCatchAbreast</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_catch_abreast.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_catch_over2underarm.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandCatchOver2Underarm</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_catch_over_2_underarm.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_catch_underarm.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandCatchUnderarm</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_catch_underarm.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_door_close_inward.png" width="18%" style="border-radius: 5px;"-->
<!--                            alt='<b>ShadowHandDoorCloseInward</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_door_close_inward.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_door_close_outward.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandDoorCloseOutward</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_door_close_outward.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_door_open_inward.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandDoorOpenInward</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_door_open_inward.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_door_open_outward.png" width="18%" style="border-radius: 5px;"-->
<!--                            alt='<b>ShadowHandDoorOpenOutward</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_door_open_outward.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_grasp_and_place.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandGraspAndPlace</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_grasp_and_place.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_kettle.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandKettle</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_kettle.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_lift_undearm.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandLiftUnderarm</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_lift_underarm.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_over.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandOver</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_over.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_pen.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandPen</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_pen.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_push_block.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandPushBlock</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_push_block.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_re_orientation.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandReorientation</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_reorientation.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_scissors.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandScissors</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_scissors.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_swing_cup.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandSwingCup</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_swing_cup.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_switch.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandSwitch</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_switch.txt' onclick="populateDemo(this, 1);">-->

<!--                        <img src="videos/task_final/bidex_two_catch_underarm.png" width="18%" style="border-radius: 5px;" alt='<b>ShadowHandTwoCatchUnderarm</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/shadow_hand_two_catch_underarm.txt' onclick="populateDemo(this, 1);">-->
<!--                    </div>-->
<!--                </div>-->
<!--                <div class="row border rounded" style="padding-top:12px; padding-bottom:12px;">-->
<!--                    <div class="col-md-6">-->
<!--                        <video id="demo-video-1" style="border-radius: 5px;" autoplay loop muted webkit-playsinline-->
<!--                            playsinline onclick="setAttribute('controls', 'true');">-->
<!--                            <source id="expandedImg-1" src="videos/placeholder.mp4" type="video/mp4">-->
<!--                        </video>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--            <div class="col-md-6">-->
<!--                <div id="imgtext-1" style="font-size: 1.5em">Select an image above:</div>-->
<!--                <div>-->
<!--                    <pre-->
<!--                        class="p-1"><code class="language-python" id="answer-1">Eureka response shown within code block.</code></pre>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--    </section>-->

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Universal Platform for Reinforcement Learning with Diverse Feedback Types</span></h2>
                        <p style="font-size: 125%">To align RLHF methodologies with practical problems and cater to researchers’ needs for systematic
studies of various feedback types within a unified context, we introduce the Uni-RLHF system. We start with the universal annotation platform, which supports various types of
human feedback along with a standardized encoding format for diverse human feedback. Using this platform, we have established a pipeline for crowdsourced feedback collection and filtering,
amassing large-scale crowdsourced labeled datasets and setting a unified research benchmark.
                        </p>
                        <br>
                        <br>

                        <h3 class="title is-4"><span class="dvima">Implementation For Multi-feedback Annotation Platform</span></h3>

                        <img src="assets/images/framework.jpg" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                        <span style="font-size: 125%">
                            <span style="font-weight: bold">Overview of the Uni-RLHF system.</span> Uni-RLHF consists of three components including the platform, the datasets, and the offline RLHF baselines. Uni-RLHF packs up
abstractions for RLHF annotation workflow, where the essentials include: ① interfaces supporting
a wide range of online environments and offline datasets, ② a query sampler that determines which
data to display, ③ an interactive user interface, enabling annotators to view available trajectory seg-
ments and provide feedback responses and ④ a feedback translator that transforms diverse feedback
labels into a standardized format.
                        </span>
                        <br>
                        <br>
                        <br>
                        <img src="assets/images/env.png" class="interpolation-image" alt=""
                            style="display: block; width: 80%; margin-left: auto; margin-right: auto" />

                        <span style="font-size: 125%">
                            <span style="font-weight: bold">The Uni-RLHF supports both online and offline training modes. Some representative tasks from these environments are visualized above.</span>
Furthermore, Uni-RLHF allows easy customization and integration of new offline datasets
by simply adding three functions.
                        </span>
                        <br>
                        <br>
                        <h3 class="title is-4"><span class="dvima">Standardized Feedback Encoding Format for Reinforcement Learning</span></h3>

                        <br>
                        <div class="columns">
                            <div class="column has-text-centered">
                                <img src="assets/images/Comparative.png" class="interpolation-image" alt="" />
                                <p style="font-size: 110%">Comparative Feedback</p>
                            </div>
                            <div class="column has-text-centered">
                                <img src="assets/images/Attribute.png" class="interpolation-image" alt="" />
                                <p style="font-size: 110%">Attribute Feedback</p>
                            </div>
                            <div class="column has-text-centered">
                                <img src="assets/images/Evaluative.png" class="interpolation-image" alt="" />
                                <p style="font-size: 110%">Evaluative Feedback</p>
                            </div>
                            <div class="column has-text-centered">
                                <img src="assets/images/Keypoint.png" class="interpolation-image" alt="" />
                                <p style="font-size: 110%">Keypoint Feedback</p>
                            </div>
                            <div class="column has-text-centered">
                                <img src="assets/images/Visual.png" class="interpolation-image" alt="" />
                                <p style="font-size: 110%">Visual Feedback</p>
                            </div>
                        </div>
                        <span style="font-size: 125%">
                            <span style="font-weight: bold">Standardized Feedback Encoding Format for Reinforcement Learning.</span> To capture and utilize diverse and heterogeneous feedback labels from annotators, we analyze a
range of research and propose a standardized feedback encoding format along with possible training methodologies.
                        </span>


                        <br>
                        <br>
                        <h3 class="title is-4"><span class="dvima">Large-scale Crowdsourced Annotation Pipeline</span></h3>

                        <span style="font-size: 125%">
                            To validate the ease of use of various aspects of the Uni-RLHF platform, we implemented large-scale crowdsourced annotation tasks for feedback label collection, using widely recognized Offline RL datasets.
                            After completing the data collection, we conducted two rounds of data filtering to minimize the amount of noisy crowdsourced data. <strong>We establish a systematic pipeline of crowdsourced annotations, resulting in large-scale annotated
                            datasets comprising more than 15 million steps across 32 popular tasks.</strong>
                            Our goal is to construct crowdsourced data annotation pipelines around the Uni-RLHF,
                            facilitating the creation of large-scale annotated datasets via parallel crowdsourced data annotation and filtering.
                        </span>

<!--                        <div class="columns">-->
<!--                            <div class="column">-->
<!--                                <img src="assets/images/eureka_pen_spinning.png" class="interpolation-image" alt="" />-->
<!--                                    &lt;!&ndash; style="display: block; width: 40%; margin-left: auto; margin-right: auto" /> &ndash;&gt;-->
<!--                                <br>-->
<!--                            </div>-->
<!--                            <div class="column has-text-centered">-->
<!--                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                                    <source src="videos/task_final/shadow_hand.mp4" type="video/mp4">-->
<!--                                </video>-->
<!--                                <p style="font-size: 110%">Pretrained Pen Reorientation</p>-->
<!--                            </div>-->
<!--                            <div class="column has-text-centered">-->
<!--                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                                    <source src="videos/pen_gallery/pen_spin_center.mp4" type="video/mp4">-->
<!--                                </video>-->
<!--                                <p style="font-size: 110%">Finetuned Pen Spinning</p>-->
<!--                            </div>-->
<!--                        </div>-->

                        <br>
                        <br>
                        <br>
                        <img src="assets/images/ann_acc2.png" class="interpolation-image" alt=""
                            style="display: block; width: 30%; margin-left: auto; margin-right: auto" />

                        <span style="font-size: 125%">
                            <span style="font-weight: bold">The effectiveness of each component in the annotation pipeline.</span> We initially sampled 300
                            trajectory segments of the left-c task in SMARTS for expert annotation, referred to as <strong>Oracle</strong>.
                            We had five crowdsourcing instances, each annotating 100 trajectories in three distinct settings.
                            <i>naive</i> implies only seeing the task description, <i>+example</i> allows for viewing five expert-provided annotation
                            samples and detailed analysis, and <i>+filter</i> adds filters to the previous conditions. The experimental results
                            displayed in above, revealed that each component significantly improved annotation accuracy, ultimately achieving a 98% agreement rate
                            with the expert annotations.
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Evaluating Benchmarks for offline RLHF</span></h2>

                        <p style="font-size: 125%">
                        Finally, we conducted numerous experiments on downstream decision-making tasks, utilizing the collected crowdsourced
                        feedback datasets to verify the reliability of the Uni-RLHF system.
                        </p>
                        <br>
                        <h3 class="title is-4"><span class="dvima">Evaluating Offline RL with Comparative Feedback</span></h3>

                        <span style="font-size: 125%">
                            <span style="font-weight: bold">D4RL Results.</span> We used <strong>Oracle</strong> to represent models trained using hand-designed task rewards.
                            In addition, we assessed two different methods of acquiring labels: one is crowd-sourced labels obtained by crowd-sourcing through
                            the Uni-RLHF system, denoted as <strong>CS</strong>, and the other is synthetic labels generated by script teachers based on ground
                            truth task rewards, which can be considered as expert labels, denoted as <strong>ST</strong>.
                        </span>
                        <br>
                        <br>
                        <img src="assets/images/d4rl2.png" class="interpolation-image" alt=""
                            style="display: block; width: 100%; margin-left: auto; margin-right: auto" />
                        <br>

                        <span style="font-size: 125%">
                            <span style="font-weight: bold">SMARTS Results.</span> We studied three typical scenarios in autonomous driving scenarios.
                            The reward function design is particularly complex because it requires more extensive expertise
                            and balancing multiple factors. We empirically demonstrate that we can achieve competitive performance simply
                            by crowdsourced annotations, compared to carefully designed reward functions or scripted teachers.
                        </span>
                        <br>
                        <br>
                        <img src="assets/images/smarts.png" class="interpolation-image" alt=""
                            style="display: block; width: 65%; margin-left: auto; margin-right: auto" />
                        <br>

                        <div class="columns">
                            <div class="column has-text-centered">
                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                                    <source src="videos/left_c.mp4" type="video/mp4">
                                </video>
                                <p style="font-size: 110%">Left_c</p>
                            </div>
                            <div class="column has-text-centered">
                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                                    <source src="videos/cutin.mp4" type="video/mp4">
                                </video>
                                <p style="font-size: 110%">Cutin</p>
                            </div>
                            <div class="column has-text-centered">
                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                                    <source src="videos/cruise.mp4" type="video/mp4">
                                </video>
                                <p style="font-size: 110%">Cruise</p>
                            </div>
                        </div>
                        <div class="columns is-centered" style="font-size: 120%">
                            <p><strong>Oracle model (left) and our CS model (right) for three scenarios in autonomous driving</strong></p>
                        </div>
                        <span style="font-size: 125%">
                            Left_c video presents an instance
                            of a left-turn-cross scenario in which the CS model succeeds, while the Oracle model fails due to a collision.
                            In this scenario, the ego vehicle is required to start from a single-lane straight road, make a left turn at the cross intersection,
                            proceed onto another two-lane straight road, and ultimately reach a goal located at the end of one of these two lanes.
                            The video reveals that the Oracle model fails to timely decelerate the ego vehicle,
                            resulting in a collision with the vehicle ahead, whereas the CS model opts to stop and wait
                            for the vehicle in front, eventually passing through the intersection smoothly.
                        </span>
                        <br>
                        <br>
                        <h3 class="title is-4"><span class="dvima">Evaluating Offline RL with Attribute Feedback</span></h3>
<!--                        <br>-->
                        <span style="font-size: 125%">
                            <span style="font-weight: bold">Trained model’s ability to switch behavior.</span> We ran the model for 1000 steps and
visualized the behavior switching by adjusting the target attributes every 200 steps, recording the walker’s speed and torso height. The attribute
values for speed were set to [0.1, 1.0, 0.5, 0.1, 1.0], and for height, they were set to [1.0, 0.6, 1.0, 0.1, 1.0]. The
corresponding changes in attributes can be clearly observed in the curves and corresponding video.
                        </span>
                        <br>
                        <br>
                        <div class="columns">
                            <div class="column has-text-centered">
                                <video poster="" id="" autoplay controls muted loop height="30%" playbackRate=2.0 style="border-radius: 0px;">
                                    <source src="videos/attribute.mp4" type="video/mp4">
                                </video>
<!--                                <p style="font-size: 110%">Walker</p>-->
                            </div>
                            <div class="rows">
                                <div class="column has-text-centered">
                                    <img src="assets/images/speed.png" class="interpolation-image" alt=""
                            style="display: block; width: 50%; margin-left: auto; margin-right: auto" />
<!--                                    <p style="font-size: 110%">Speed</p>-->
                                </div>
                                <div class="column has-text-centered">
                                    <img src="assets/images/height.png" class="interpolation-image" alt=""
                            style="display: block; width: 50%; margin-left: auto; margin-right: auto" />
<!--                                    <p style="font-size: 110%">Torso height</p>-->
                                </div>
                            </div>

                        </div>

                        <span style="font-size: 125%">
                            <span style="font-weight: bold">Human Evaluation.</span> we conducted human evaluation experiments for
                            the agents trained by attribute feedback. We first collected 3 trajectories with different humanness,
                            which were set to [0.1, 0.6, 1] and invited five human evaluators. The human evaluators performed
                            a blind selection to judge which video is most human-like and which video is least human-like.
                        </span>

                        <br>
                        <br>
                        <br>
                        <div class="columns is-centered has-text-centered">
                            <video poster="" id="" autoplay controls muted width="40%" playbackRate=2.0 style="border-radius: 5px;">
                                <source src="./videos/humanhuman.mp4" type="video/mp4">
                            </video>
                        </div>
                        <br>

                        <span style="font-size: 125%">
                            <span style="font-weight: bold"></span> Finally, all people correctly chose the highest humanness trajectory, and only one person chose
the lowest humanness trajectory incorrectly. The experimental results confirm that agents through
RLHF training are able to follow the abstraction metrics well.
                        </span>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Online Experiments</span></h2>

                        <p style="font-size: 125%">
                        We validate the effectiveness of the online mode of Uni-RLHF, which allow agents to learn novel behaviors
                        where a suitable reward function is difficult to design. Finally, we give totally 200 queries of human
                        feedback for walker front flips experiments and we observe that <strong>walker can master the continuous multiple
                        front flip fluently</strong>.
                        </p>
                        <br>
                        <br>
                        <div class="columns is-centered has-text-centered">
                        <video poster="" id="" autoplay controls muted width="20%" playbackRate=2.0 style="border-radius: 5px;">
                            <source src="./videos/walker.mp4" type="video/mp4">
                        </video>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Visualization of the Platform</span></h2>

                        <div class="columns">
                            <div class="column has-text-centered">
                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                                    <source src="videos/uni1.mp4" type="video/mp4">
                                </video>
                                <p style="font-size: 110%">Annotation</p>
                            </div>
                            <div class="column has-text-centered">
                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                                    <source src="videos/uni2.mp4" type="video/mp4">
                                </video>
                                <p style="font-size: 110%">Create Task</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

<!--    <section class="section">-->
<!--        <div class="container is-max-widescreen">-->
<!--            <div class="rows">-->
<!--                <div class="rows is-centered ">-->
<!--                    <div class="row is-full-width">-->

<!--                        <div class="columns is-centered has-text-centered">-->
<!--                            <video poster="" id="" autoplay controls muted loop width="70%" height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                                <source src="videos/eureka_env_context.mp4" type="video/mp4">-->
<!--                            </video>-->
<!--                        </div>-->
<!--                        <span style="font-size: 125%">-->
<!--                        <span style="font-weight: bold">Environment as Context.</span> -->
<!--                        By using the raw environment code as context, Eureka can zero-shot generate plausible reward programs, without any task-specific prompt engineering. This allows Eureka to be a <b>generalist</b> reward designer,-->
<!--                         readily producing reward functions on first try for all our environments.                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <div class="columns is-centered has-text-centered">-->
<!--                        <video poster="" id="" autoplay controls muted loop width="70%" height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                            <source src="videos/shadow_hand_multi.mp4" type="video/mp4">-->
<!--                        </video>-->
<!--                        </div>-->
<!--                        <span style="font-size: 125%">-->
<!--                        <span style="font-weight: bold">Rapid Reward Evaluation via Massively Parallel RL.</span> -->
<!--                        Leveraging state-of-the-art GPU-accelerated simulation in NVIDIA Isaac Gym, Eureka is able to quickly evaluate -->
<!--                        the quality of a large batch of reward candidates, enabling scalable search in the reward function space.-->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <div class="columns is-centered has-text-centered">-->
<!--                        <video poster="" id="" autoplay controls muted loop width="70%" height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                            <source src="videos/eureka_reward_reflection.mp4" type="video/mp4">-->
<!--                        </video>-->
<!--                        </div>-->
<!--                        &lt;!&ndash; <img src="assets/images/reward_diff.png" class="interpolation-image" alt="" &ndash;&gt;-->
<!--                        &lt;!&ndash; style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> &ndash;&gt;-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold">Eureka Reward Reflection.</span> After reward evaluation, -->
<!--                            Eureka constructs reward reflection that summarizes the key statistics of the RL training. Then, Eureka uses this reward reflection -->
<!--                            to enable the backbone LLM (GPT-4) to flexibly improve the reward functions with many distinct-->
<!--                            types of free-form, targeted modification, such as (1) changing the hyperparameter of existing-->
<!--                            reward-->
<!--                            components, (2)-->
<!--                            changing the functional form of existing reward components, and (3) introducing new-->
<!--                            reward-->
<!--                            components.</span>-->
<!--                        </div>-->
<!--                </div>-->

<!--            </div>-->
<!--        </div>-->
<!--    </section>-->

<!--    <section class="section">-->
<!--        <div class="container is-max-widescreen">-->
<!--            <div class="rows">-->
<!--                <div class="rows is-centered ">-->
<!--                    <div class="row is-full-width">-->
<!--                        <h2 class="title is-3"><span class="dvima">Experiments</span></h2>-->
<!--                        <p style="font-size: 125%">-->
<!--                            We thoroughly evaluate Eureka on a diverse suite of robot embodiments and tasks,-->
<!--                            testing its-->
<!--                            ability-->
<!--                            to generate reward functions, solve new tasks, and incorporate various forms of-->
<!--                            human input.-->
<!--                        </p>-->
<!--                        <br>-->
<!--                        <p style="font-size: 125%">-->
<!--                            Our environments consist of 10 distinct robots and 29 tasks implemented using the-->
<!--                            IsaacGym simulator. First, we include 9 original environments from-->
<!--                            IsaacGym (Isaac), covering a diverse set of robot morphologies from quadruped,-->
<!--                            bipedal,-->
<!--                            quadrotor,-->
<!--                            cobot arm, to dexterous hands. In addition to coverage over robot form factors, we-->
<!--                            ensure-->
<!--                            depth in our-->
<!--                            evaluation by including all 20 tasks from the Bidexterous Manipulation (Dexterity)-->
<!--                            benchmark.-->
<!--                            Dexterity contains 20 complex bi-manual tasks that require a pair of Shadow Hands to-->
<!--                            solve a wide range of complex manipulation skills, ranging from object handover to-->
<!--                            rotating-->
<!--                            a cup by-->
<!--                            180 degrees-->
<!--                        </p>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <h3 class="title is-4"><span class="dvima">Evaluation Results</span></h3>-->

<!--                        <img src="assets/images/eureka_bar_chart.png" class="interpolation-image" alt=""-->
<!--                            style="display: block; margin-left: auto; margin-right: auto" />-->
<!--                        <br>-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold">Eureka can generate super human-level reward functions.</span> Across 29 tasks, Eureka rewards outperform expert human-written ones on 83% of them with an average normalized improvement of 52%.-->
<!--                             In particular, Eureka realizes much greater gains on high-dimensional-->
<!--                            dexterity-->
<!--                            environments.-->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <img src="assets/images/eureka_improvement.png" class="interpolation-image" alt=""-->
<!--                            style="display: block; width: 60%; margin-left: auto; margin-right: auto" />-->
<!--                        <br>-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold">Eureka evolutionay reward search enables consistent reward improvement over time.</span> Eureka progressively produces better-->
<!--                            rewards that eventually exceed human-level by combining large-scale reward search with detailed reward reflection feedback.-->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        &lt;!&ndash; <h3 class="title is-4"><span class="dvima">Comparison with Human Rewards</span></h3> &ndash;&gt;-->
<!--                        <br>-->

<!--                        <img src="assets/images/isaac_eureka_correlation.png" class="interpolation-image" alt=""-->
<!--                            style="display: block; width: 40%; margin-left: auto; margin-right: auto" />-->
<!--                        <br>-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold">Eureka generates novel rewards.</span>  We -->
<!--                            assess the novelty of Eureka rewards by computing the correlations between Eureka and human rewards on all Isaac tasks.-->
<!--                            As shown, Eureka mostly generates weakly correlated reward functions that outperform the human ones. In addition, we observe that -->
<!--                            <b>the harder the task is, the less correlated the Eureka rewards.</b> In a few cases, Eureka rewards are even negatively correlated with human rewards while significantly -->
<!--                            outperforming them. -->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <h3 class="title is-4"><span class="dvima">Dexterous Pen Spinning via Curriculum Learning</span></h3>-->

<!--                        <br>-->

<!--                        <div class="columns">-->
<!--                            <div class="column">-->
<!--                                <img src="assets/images/eureka_pen_spinning.png" class="interpolation-image" alt="" />-->
<!--                                    &lt;!&ndash; style="display: block; width: 40%; margin-left: auto; margin-right: auto" /> &ndash;&gt;-->
<!--                                <br>-->
<!--                            </div>-->
<!--                            <div class="column has-text-centered">-->
<!--                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                                    <source src="videos/task_final/shadow_hand.mp4" type="video/mp4">-->
<!--                                </video>-->
<!--                                <p style="font-size: 110%">Pretrained Pen Reorientation</p>-->
<!--                            </div>-->
<!--                            <div class="column has-text-centered">-->
<!--                                <video poster="" id="" autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">-->
<!--                                    <source src="videos/pen_gallery/pen_spin_center.mp4" type="video/mp4">-->
<!--                                </video>-->
<!--                                <p style="font-size: 110%">Finetuned Pen Spinning</p>-->
<!--                            </div>-->
<!--                        </div>-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold"></span> The pen spinning task requires a Shadow Hand to continuously rotate a pen to achieve some pre-defined spinning patterns for -->
<!--                            as many cycles as possible. We solve this task by (1) instructing Eureka to generate a reward function for re-orienting the pen to random target configurations,-->
<!--                            and then (2) fine-tuning this pre-trained policy using the Eureka reward to reach the desired sequence of pen-spinning configurations.-->
<!--                            As shown, Eureka fine-tuning quickly adapts the policy successfully spin the pen for many cycles in a row. In contrast, neither pre-trained or learning-from-scratch -->
<!--                            policies can complete even a single cycle.-->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <h3 class="title is-3"><span class="dvima">Eureka from Human Feedback</span></h3>-->
<!--                        <br>-->

<!--                        <img src="assets/images/bidex_reward_assistant.png" class="interpolation-image" alt=""-->
<!--                            style="display: block; width: 70%; margin-left: auto; margin-right: auto" />-->
<!--                        <br>-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold">Eureka effectively improves and benefits from human reward initialization.</span> -->
<!--                        We study whether starting with a human reward function initialization, a common scenario in real-world RL applications, is advantageous for Eureka. -->
<!--                        As shown, regardless of the quality of the human rewards, Eureka improves and benefits from human rewards as Eureka (Human Init.) is uniformly better than both Eureka and Human on all tasks.-->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->

<!--                        <img src="assets/images/eureka_rlhf.png" class="interpolation-image" alt=""-->
<!--                            style="display: block; width: 100%; margin-left: auto; margin-right: auto" />-->
<!--                        <br>-->
<!--                        <span style="font-size: 125%">-->
<!--                            <span style="font-weight: bold">Eureka enables In-Context Reinforcement Learning from Human Feedback (RLHF).</span> Eureka can incorporate human feedback to modify its rewards so that they progressively induce safer and more human-aligned agent behavior. -->
<!--                            In this example, we show how Eureka can teach a Humanoid how to run upright from a handful of human feedback, which replaces the previous automated reward reflection.-->
<!--                             The final learned behavior (Iteration 5) is more preferred by human users by a wide margin than -->
<!--                            the original Eureka-learned Humanoid running gait. -->
<!--                        </span>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                        <br>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--    </section>-->

<!--    <section class="section">-->
<!--        <div class="container is-max-widescreen">-->
<!--            <div class="columns" style="align-items: end;">-->
<!--                <div class="column is-half">-->
<!--                    <div class="col-md-4 col-sm-4 col-xs-4">-->
<!--                        <img src="videos/humanoid_rlhf/humanoid-step0.png" width="40%" style="border-radius: 5px;" alt='<b>Iteration 1</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/rlhf_rewards/humanoid_step0.txt' onclick="populateDemo(this, 2);">-->

<!--                        <img src="videos/humanoid_rlhf/humanoid-step1.png" width="40%" style="border-radius: 5px;" alt='<b>Iteration 2</b>, Human feedback: <br>-->
<!--                        <span style="font-size: 0.8em; line-height: 30px;">The learned behavior resembles forward squat jump; -->
<!--                        please revise the reward function so that the behavior resembles forward running.</span><br><br>-->
<!--                        <b>Iteration 2</b>, Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/rlhf_rewards/humanoid_step1.txt' onclick="populateDemo(this, 2);">-->

<!--                        <img src="videos/humanoid_rlhf/humanoid-step2.png" width="40%" style="border-radius: 5px;" alt='<b>Iteration 3</b>, Human feedback: <br>-->
<!--                        <span style="font-size: 0.8em; line-height: 30px;">The learned behavior now looks like duck walk; -->
<!--                            the legs are indeed alternating but the torso is very low. -->
<!--                            Could you improve the reward function for upright running?</span><br><br>-->
<!--                        <b>Iteration 3</b>, Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/rlhf_rewards/humanoid_step2.txt' onclick="populateDemo(this, 2);">-->

<!--                        <img src="videos/humanoid_rlhf/humanoid-step3.png" width="40%" style="border-radius: 5px;" alt='<b>Iteration 4</b>, Human feedback: <br>-->
<!--                        <span style="font-size: 0.8em; line-height: 30px;">The learned behavior has the robot hopping on one of its foot in order to move forward.-->
<!--                            Please revise the reward function to encourage upright running behavior.</span><br><br>-->
<!--                        <b>Iteration 4</b>, Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/rlhf_rewards/humanoid_step3.txt' onclick="populateDemo(this, 2);">-->

<!--                        <img src="videos/humanoid_rlhf/humanoid-step4.png" width="40%" style="border-radius: 5px;" alt='<b>Iteration 5</b>, Human feedback: <br>-->
<!--                        <span style="font-size: 0.8em; line-height: 30px;">This reward function removed the penalty for low torse position that you added last time; could you just add it back in? </span><br><br>-->
<!--                        <b>Iteration 5</b>, Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/rlhf_rewards/humanoid_step4.txt' onclick="populateDemo(this, 2);">-->

<!--                        <img src="videos/humanoid_rlhf/humanoid-norlhf.png" width="40%" style="border-radius: 5px;" alt='<b>Eureka without RLHF</b>, best Eureka reward:-->
<!--                        [sep]-->
<!--                        assets/reward_functions/humanoid.txt' onclick="populateDemo(this, 2);">-->

<!--                    </div>-->
<!--                </div>-->
<!--                <div class="row border rounded" style="padding-top:12px; padding-bottom:12px;">-->
<!--                    <div class="col-md-6">-->
<!--                        <video id="demo-video-2" style="border-radius: 5px;" autoplay loop muted webkit-playsinline-->
<!--                            playsinline onclick="setAttribute('controls', 'true');">-->
<!--                            <source id="expandedImg-2" src="videos/placeholder.mp4" type="video/mp4">-->
<!--                        </video>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--            <div class="col-md-6">-->
<!--                <div id="imgtext-2" style="font-size: 1.5em">Select an image above:</div>-->
<!--                <div>-->
<!--                    <pre-->
<!--                        class="p-1"><code class="language-python" id="answer-2">Eureka response shown within code block.</code></pre>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--    </section>-->

   

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{yuan2023unirlhf,
    title={Uni-{RLHF}: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback},
    author={Yuan, Yifu and Hao, Jianye and Ma, Yi and Dong, Zibin and Liang, Hebin and Liu, Jinyi and Feng, Zhixin and Zhao, Kai and Zheng, Yan}
    booktitle={The Twelfth International Conference on Learning Representations, ICLR},
    year={2024},
    url={https://openreview.net/forum?id=WesY0H9ghM},
}
</code></pre>
    </div>
</section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>,
                            <a href="https://vimalabs.github.io/">VIMA</a>, and <a
                                href="https://language-to-reward.github.io/">L2R</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

<script>

    timeoutIds = [];

    function populateDemo(imgs, num) {
        // Get the expanded image
        var expandImg = document.getElementById("expandedImg-" + num);
        // Get the image text
        var imgText = document.getElementById("imgtext-" + num);
        var answer = document.getElementById("answer-" + num);

        // Use the same src in the expanded image as the image being clicked on from the grid
        expandImg.src = imgs.src.replace(".png", ".mp4");
        var video = document.getElementById('demo-video-' + num);
        // or video = $('.video-selector')[0];
        video.pause()
        video.load();
        video.play();
        video.removeAttribute('controls');

        console.log(expandImg.src);
        // Use the value of the alt attribute of the clickable image as text inside the expanded image
        var qa = imgs.alt.split("[sep]");
        imgText.innerHTML = qa[0];
        answer.innerHTML = "";
        // Show the container element (hidden with CSS)
        expandImg.parentElement.style.display = "block";
        for (timeoutId of timeoutIds) {
            clearTimeout(timeoutId);
        }

        // NOTE (wliang): Modified from original to read from file instead
        fetch(qa[1])
            .then(response => response.text())
            .then(contents => {
                // Call the processData function and pass the contents as an argument
                typeWriter(contents, 0, qa[0], num);
            })
            .catch(error => console.error('Error reading file:', error));
    }

    function typeWriter(txt, i, q, num) {
        var imgText = document.getElementById("imgtext-" + num);
        var answer = document.getElementById("answer-" + num);
        if (imgText.innerHTML == q) {
            for (let k = 0; k < 5; k++) {
                if (i < txt.length) {
                    if (txt.charAt(i) == "\\") {
                        answer.innerHTML += "\n";
                        i += 1;
                    } else {
                        answer.innerHTML += txt.charAt(i);
                    }
                    i++;
                }
            }
            hljs.highlightAll();
            timeoutIds.push(setTimeout(typeWriter, 1, txt, i, q, num));
        }
    }

</script>

</html>